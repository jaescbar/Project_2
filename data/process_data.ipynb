{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline\n",
    "\n",
    "In this document is the ETL process, where the data is extract, transformed and load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries\n",
    "\n",
    "Import needed libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract data\n",
    "\n",
    "This part loads and merges data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(messages_filepath, categories_filepath):\n",
    "    \n",
    "    '''\n",
    "    load_data loads and merges data from message and categories \n",
    "    \n",
    "    INPUT\n",
    "    messages_filepath: Path of database containing text features (csv)\n",
    "    \n",
    "    categories_filepath: Path of database containing categories features (csv)\n",
    "    \n",
    "    OUTPUT\n",
    "    merged DataFrame\n",
    "    \n",
    "    '''\n",
    "\n",
    "    messages = pd.read_csv(messages_filepath)\n",
    "    categories = pd.read_csv(categories_filepath)\n",
    "    df = messages.merge(categories, \\\n",
    "                        on=['id'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clean Data\n",
    "\n",
    "This parte cleans and prepares data to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \n",
    "    '''    \n",
    "    clean_data cleans and prepares data to be saved. Splits categories into separate columns\n",
    "    creates dummy variables and removes duplicates\n",
    "    \n",
    "    INPUT\n",
    "    df: merged df\n",
    "    \n",
    "    OUTPUT\n",
    "    clean and ready to use df\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # create categories subset with new column names\n",
    "    categories = df['categories'].str.split(pat=';', n=-1, expand=True)\n",
    "    row = categories.iloc[0]\n",
    "    category_colnames = row.apply(lambda x: x[:-2])\n",
    "    categories.columns = category_colnames\n",
    "    \n",
    "    \n",
    "    # transform columns so that each is a dummy variable\n",
    "    for column in categories:\n",
    "        categories[column] = categories[column].apply(lambda x:x[-1:])\n",
    "        \n",
    "    categories =categories.apply(pd.to_numeric)\n",
    "\n",
    "    \n",
    "    # drop original category column and concatenate original DataFrame with categories DataFrame \n",
    "    df.drop('categories', axis=1, inplace=True)\n",
    "    df = pd.concat([df, categories], axis=1)\n",
    "    \n",
    "    \n",
    "    #drop duplicates\n",
    "    df.drop_duplicates(subset='id', inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load\n",
    "\n",
    "This part exports the final transformed DataFrame to SQLite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df, database_filename):\n",
    "    \n",
    "    '''\n",
    "    save_data exports the transformed DataFrame to SQL\n",
    "    \n",
    "    INPUT\n",
    "    df\n",
    "    database_filename\n",
    "    \n",
    "    '''\n",
    "    database_filepath = database_filename\n",
    "    engine = create_engine('sqlite:///' + database_filepath)\n",
    "    df.to_sql('Database Project_2', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if len(sys.argv) == 4:\n",
    "\n",
    "        messages_filepath, categories_filepath, database_filepath = sys.argv[1:]\n",
    "\n",
    "        print('Loading data...\\n    MESSAGES: {}\\n    CATEGORIES: {}'\n",
    "              .format(messages_filepath, categories_filepath))\n",
    "        df = load_data(messages_filepath, categories_filepath)\n",
    "\n",
    "        print('Cleaning data...')\n",
    "        df = clean_data(df)\n",
    "        \n",
    "        print('Saving data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "        save_data(df, database_filepath)\n",
    "        \n",
    "        print('Cleaned data saved to database!')\n",
    "    \n",
    "    else:\n",
    "        print('Please provide the filepaths of the messages and categories '\\\n",
    "              'datasets as the first and second argument respectively, as '\\\n",
    "              'well as the filepath of the database to save the cleaned data '\\\n",
    "              'to as the third argument. \\n\\nExample: python process_data.py '\\\n",
    "              'disaster_messages.csv disaster_categories.csv '\\\n",
    "              'DisasterResponse.db')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
